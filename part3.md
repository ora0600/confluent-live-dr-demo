# Cluster Setup with an active DR design for the Apps Design

here we cover
* active-passive Cluster DR design
* active-active Cluster DR design

Both are working with Cluster Linking (Replicator is not covered in this repo).

# Create Dedicated Cluster setup with Cluster Links

We use the same OrgAdmin API Key mentioned in [Part1: Create Cloud API Keys and Service Account](part1.pmd).
Store the API key in Environment Variables:

```bash
cd Part3/01-kafka-ops-team
# SA tf_cmrunner_cmue
export TF_VAR_confluent_cloud_api_key="KEYXXXXXXXXXXXXX"
export TF_VAR_confluent_cloud_api_secret="SECRETYYYYYXXXXXXYXYXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
``` 

Or store Key and secret into a file `terraform.tfvars` (will be read by terraform automatically)

```bash
cd Part3/01-kafka-ops-team
cat > terraform.tfvars <<EOF
confluent_cloud_api_key = "KEYXXXXXXXXXXXXX"
confluent_cloud_api_secret = "SECRETYYYYYXXXXXXYXYXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
EOF
``` 

terraform is now able to communicate with the Confluent Cloud API and will create a couple of resources.

* Create 1 environments `cmprod-active`, this time I have both clusters in one environment. Typically you will use 2 environment, because the Schema Reqistry would also effected in DR case, and Schema can be sync into a new region with Schema Linking.
* Create new Env Manager Service Account with API Keys

![Cloud resources to be created in Part3](img/part3_resouces.png)

Start terraform now:

```bash
# we are in this folder Part3/01-kafka-ops-team
# First act as Ops team, create Environment and env-drmanager
terraform init
terraform plan
terraform apply --auto-approve
# Output
# Apply complete! Resources: 6 added, 0 changed, 0 destroyed.
# Outputs:
# envid = "env-XXXX"
# key = "YYYYYYYYYYYYYY"
# resource-ids = <sensitive>
# secret = <sensitive>
terraform output resource-ids
```

The `terraform apply` will generate a file:`env-source` , which will be copied into the product team folder `Part3/02-env-admin-product-team`. This file will be used by product team, which includes all they need to work with Confluent Cloud. (you need to source it soon). They will operate as Delegated Admin for the created environment.

## Build the cloud resource as a product team environment admin 

Now, you can act a Admin Product Team with the Service Account `cmenv-drmanager`. This was pretty fresh created (see section above).
Create the Prod-Cluster setup including the DR cluster and 2 cluster links now:

In this lab we will create a active and passive DR Clusters with Cluster Linking. This cluster is from type dedicated and costs more money than basic. Please check [price list](https://www.confluent.io/confluent-cloud/pricing/) before continue. 

```bash
cd ../02-env-admin-product-team
# source envid and API key of env-manager file was generated by terraform
source env-source
# Create cloud resources
terraform init
terraform plan
terraform apply --auto-approve
terraform output resource-ids
```

Two dedicated cluster in two different regions and cloud providers are created.
![DR Design clusters](img/clusters.png)

These clusters are linked with two cluster links.
![DR Design clusters](img/cluster_links.png)

Now, you can start the clients:
```bash
./00_start_clients.sh
```

## Active-Passive Cluster setup

Two Clusters from type dedicated are running in environment cmprod-active.

Both dedicated clusters are with Public network endpoints, a test topic in source cluster , a one-directional-cluster-link between both clusters and a mirror topic on the test topic.
The cluster link will get the following config set:
* consumer.offset.sync.enable=true
* consumer.offset.group.filters={"groupFilters": [{"name": "*","patternType": "LITERAL","filterType": "INCLUDE"}]}
* consumer.offset.sync.ms=1000
* acl.sync.enable=true
* acl.sync.ms=1000
* acl.filters={ "aclFilters": [ { "resourceFilter": { "resourceType": "any", "patternType": "any" }, "accessFilter": { "operation": "any", "permissionType": "any" } } ] }
* topic.config.sync.ms=1000

The idea now is not to copy the complete content from primary_cluster to secondary_cluster. We run special SLA for special resources.
Only Gold SLA Topics should be in our DR setup. These are topics with a high requirement regarding downtime and HA in general.
There are couple of resources (topics) marked as GOLD and documented in `cat Part3/active-passive/scripts/sla_gold_topics.txt`. Only these resources will be mirrored in secondary_cluster.

Running the SLA based DR Plan: 
In our case we do have only one topic which needs to be geo-replicated because of high HA requirements.

```bash
cd ../active-passive/
source env-destination 
./scripts/00_create_mirror_topics.sh
```

Now, you can play a little bit in the Cloud UI. This is quite interesting. Check:
* Cluster Linking Menu (left side)
* Show Topics in DR Cluster, is data coming from cmprod_primary_cluster?
* Is client running now on seconday cluster?
* etc.

Check producer and data consumer is now running on mirrored topic in secondary_cluster:
```bash
# Check client is now running on secondary cluster
kubectl logs prod-cloudconsumercmorders-0 -n confluent
# List topics in secondary cluster
./00_list_all_topics.sh
# check if offset on secondary cluster looks good
./00_run_offset_monitor.sh
# run confluent cli
confluent kafka mirror describe cmorders --cluster $cluster_destination_id --environment $destination_envid --link passive-primary-secondary-cluster
```

### Fail Forward or Fail-and-stay

Now, we have situation after failover. In a cloud native environment we would recommend to fail forward. This means

* the DR region becomes the new Primary region.
* cloud regions offer identical service
* all applications & data systems are now in the DR region
* failing back would introduce risk with little benefit

To fail forward, simply:

* Delete topics on original cluster (or spin up new cluster)
* Establish cluster link in reverse direction (or bidirectional cluster link was created, not here. see active-active)

### Failback

Another Alternative would be to fail back to prod-cluster from DR-cluster.

1. Delete topics on Primary cluster (or spin up a new cluster)
2. Establish a cluster link in the reverse direction
3. When Primary has caught up, migrate producers & consumers back:
    a. Stop clients
    b. promote mirror topic(s)
    c. Restart clients pointed at Primary cluster

### Recovery After a Disaster

Cluster linking is asynchronous, that's why it could be that not all make it over to the DR cluster.
If the prod cluster will come back (temp outage) and there could be some data which were never replicated

1. Find the offsets at which the mirror topics were failed over.

```bash
source env-destination
confluent kafka mirror describe cmorders --cluster $cluster_destination_id --environment $destination_envid --link passive-primary-secondary-cluster
#  Link Name      |Mirror Topic| Source Topic | Mirror Status | Status Time (ms) | Partition | Partition Mirror Lag | Last Source Fetch Offset  
#-----------------+------------+--------------+---------------+------------------+-----------+----------------------+--------------------------
#  passive-primary| cmorders   | cmorders     | STOPPED       |    1708443298951 |         0 |                    0 |                     1368  
#   -secondary-cluster 
```

2. With those offsets in hand, point consumers at your original cluster, and reset their offsets to those you found in. Your consumers can then recover the data and act on it as appropriate for their application. Or use replicator to add data into DR cluster topics

## Active-Active Cluster setup

First switch back clients to Primary Cloud:

```bash
cd Part3/active-passive
./01_switchback_client.properties.sh 
kubectl get pods -n confluent
kubectl logs prod-cloudconsumercmorders-0 -n confluent
```

For the active-active Part we use bi-directional Cluster Link `active-primary-secondary-cluster`.

The cluster link will get the following config set:
* consumer.offset.sync.enable=true
* consumer.offset.group.filters={"groupFilters": [{"name": "*","patternType": "LITERAL","filterType": "INCLUDE"}]}
* consumer.offset.sync.ms=1000
* acl.sync.enable=false (not allowed with prefix)
* acl.sync.ms=1000
* acl.filters={ "aclFilters": [ { "resourceFilter": { "resourceType": "any", "patternType": "any" }, "accessFilter": { "operation": "any", "permissionType": "any" } } ] }
* topic.config.sync.ms=1000
* cluster.link.prefix=mirror-

```bash
cd ../active-active
source env-destination
confluent kafka link describe active-primary-secondary-cluster --cluster $cluster_source_id --environment $source_envid
```

The idea now is follow that [tutorial](https://docs.confluent.io/cloud/current/multi-cloud/cluster-linking/dr-failover.html#active-active-tutorial) in a script.
* Create writable topics
    a. prod: cmorders on dr: cmorder
* Create one cluster link in bidirectional mode

The idea is the same we do not copy the complete content from prod_cluster to active DR cluster and vice verso. We have special SLA for special resources. We call it Gold SLA.
There are couple resources marked as GOLD  and documented in `cat scripts/sla_gold_topics.txt`. Only these resources will be mirrored in on both clusters.

Running the SLA based DR Plan: 
In our case we do have only one topic which needs to be geo-replicated because of high HA requirements.

```bash
./scripts/00_create_mirror_topics.sh
kubectl logs dr-cloudconsumercmcustomers-primary-0 -n confluent
kubectl logs dr-cloudconsumercmcustomers-secondary-0 -n confluent
``` 

We did setup an active-active disaster recovery setup for cmcustomer topic.
![DR setup](img/cmcustomer_dr_setup.png)


Now, you can play a little bit in the Cloud UI. This is quite interesting. Check:
* Cluster Linking Menu (left side)
* Show Topics in DR Cluster, is data coming from cmprod_cluster?
* etc.

Play-Around with active-active setup now. See [docu](https://docs.confluent.io/cloud/current/multi-cloud/cluster-linking/dr-failover.html#active-active-tutorial) 

# Destroy dedicated cluster setup

## Clients

```bash
cd Part3/
kubectl get pods -n confluent
kubectl delete  -f prod-cloudconsumercmorders.yaml  --namespace confluent
kubectl delete  -f dr-cloudproducercmcustomers-primary.yaml  --namespace confluent
kubectl delete  -f dr-cloudproducercmcustomers-secondary.yaml  --namespace confluent
kubectl delete  -f prod-cloudproducercmorders.yaml  --namespace confluent
kubectl delete  -f dr-cloudconsumercmcustomers-primary.yaml  --namespace confluent
kubectl delete  -f dr-cloudconsumercmcustomers-secondary.yaml  --namespace confluent
# all pods should be terminated or in termination
kubectl get pods -n confluent
# Secrets
kubectl get secrets -n confluent
kubectl delete secret kafka-client-consumer-config-secure -n confluent
kubectl delete secret kafka-client-producer-config-secure -n confluent
kubectl delete secret dr-kafka-client-producer-config-secure -n confluent
kubectl delete secret dr-kafka-client-consumer-config-secure -n confluent
kubectl delete secret dr-kafka-client-consumer-config-secure-primary  -n confluent
kubectl delete secret dr-kafka-client-producer-config-secure-primary -n confluent
kubectl delete secret dr-kafka-client-producer-config-secure-secondary -n confluent
kubectl delete secret dr-kafka-client-consumer-config-secure-secondary -n confluent
kubectl get secrets -n confluent
```

## Confluent cloud cluster

```bash
cd 02-env-admin-product-team/
source env-source
# delete the mirror topics first in Cloud UI: Look at each link
terraform destroy
# Environments
cd ../01-kafka-ops-team/
terraform destroy
```

When you got no errors all Confluent Cloud resources should be deleted.

back to [main Readme](ReadME.md)